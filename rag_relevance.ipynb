{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQhXmcSInkfB"
      },
      "outputs": [],
      "source": [
        "!pip install datasets accelerate peft bitsandbytes trl flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "base_model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "groundedai_eval_id = \"grounded-ai/phi3-rag-relevance-judge\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(groundedai_eval_id)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
        "model_peft = PeftModel.from_pretrained(base_model, groundedai_eval_id, config=config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "merged_model = model_peft.merge_and_unload()\n",
        "merged_model.to('cuda')"
      ],
      "metadata": {
        "id": "ZXrCkdtXSwEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(text, query):\n",
        "    input = f\"\"\"\n",
        "      You are comparing a reference text to a question and trying to determine if the reference text\n",
        "  contains information relevant to answering the question. Here is the data:\n",
        "      [BEGIN DATA]\n",
        "      ************\n",
        "      [Question]: {query}\n",
        "      ************\n",
        "      [Reference text]: {text}\n",
        "      ************\n",
        "      [END DATA]\n",
        "  Compare the Question above to the Reference text. You must determine whether the Reference text\n",
        "  contains information that can answer the Question. Please focus on whether the very specific\n",
        "  question can be answered by the information in the Reference text.\n",
        "  Your response must be single word, either \"relevant\" or \"unrelated\",\n",
        "  and should not contain any text or characters aside from that word.\n",
        "  \"unrelated\" means that the reference text does not contain an answer to the Question.\n",
        "  \"relevant\" means the reference text contains an answer to the Question.\"\"\"\n",
        "    return input"
      ],
      "metadata": {
        "id": "hWTjA92botjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=merged_model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}"
      ],
      "metadata": {
        "id": "EFyeuAG0n__w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1672519-5869-4521-9ca9-28ebeb464b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check model can still perform general tasks"
      ],
      "metadata": {
        "id": "p22V7BW_G1t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": 'Why is the sky blue?'}\n",
        "]\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "output = pipe(messages, **generation_args)"
      ],
      "metadata": {
        "id": "351MnJ9prVlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ed6a8d-51da-4173-a726-57478a050105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output[0]['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "k7byF1K2rnPZ",
        "outputId": "90081cfd-0fef-4c10-aebd-47673ff2fd0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The sky appears blue to the human eye because of the way Earth's atmosphere scatters sunlight. Sunlight is made up of different colors of light, which are scattered in all directions by the gases and particles in the Earth's atmosphere. Blue light is scattered more than other colors because it travels as shorter, smaller waves. This phenomenon is known as Rayleigh scattering. When we look at the sky away from the sun, we see more scattered blue light, which is why the sky appears blue during the day.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_merged_model(text, query):\n",
        "  input = format_input(query, text)\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": input}\n",
        "  ]\n",
        "\n",
        "  pipe = pipeline(\n",
        "      \"text-generation\",\n",
        "      model=merged_model,\n",
        "      tokenizer=tokenizer,\n",
        "  )\n",
        "\n",
        "  generation_args = {\n",
        "      \"max_new_tokens\": 4,\n",
        "      \"return_full_text\": False,\n",
        "      \"temperature\": 0.01,\n",
        "      \"do_sample\": True,\n",
        "  }\n",
        "\n",
        "  output = pipe(messages, **generation_args)\n",
        "  torch.cuda.empty_cache()\n",
        "  return output[0]['generated_text'].strip().lower()"
      ],
      "metadata": {
        "id": "hWZe20YLsOio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"What is quanitzation?\", \"Tell me about 4, 8, and 16 bit quantization.\", \"What is intel?\", \"How many parameters does Claude v3 have?\"]"
      ],
      "metadata": {
        "id": "naNqKRbzdEfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for q in questions:\n",
        "  result = run_merged_model(\"\"\"How to further reduce GPU memory required for Llama 2 70B?\n",
        "    Quantization is a method to reduce the memory footprint. Quantization is able to do this by reducing the precision of the model's parameters from floating-point to lower-bit representations,\n",
        "    such as 8-bit integers. This process significantly decreases the memory and computational requirements, enabling more efficient deployment of the model, particularly on devices with limited resources.\n",
        "    However, it requires careful management to maintain the model's performance, as reducing precision can potentially impact the accuracy of the outputs.\n",
        "    In general, the consensus seems to be that 8 bit quantization achieves similar performance to using 16 bit. However, 4 bit quantization could have a noticeable impact to the model performance.\"\"\", q)\n",
        "  print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jGEZibLbbKJ",
        "outputId": "5103bbf2-3703-405b-d15c-b15c2204ee99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relevant\n",
            "relevant\n",
            "unrelated\n",
            "unrelated\n"
          ]
        }
      ]
    }
  ]
}