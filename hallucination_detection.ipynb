{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets accelerate peft bitsandbytes trl==0.8.6 flash-attn"
      ],
      "metadata": {
        "id": "HHN0yecnFeCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "base_model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "groundedai_eval_id = \"grounded-ai/phi3-hallucination-judge\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(groundedai_eval_id)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
        "model_peft = PeftModel.from_pretrained(base_model, groundedai_eval_id, config=config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "merged_model = model_peft.merge_and_unload()\n",
        "merged_model.to('cuda')"
      ],
      "metadata": {
        "id": "bMnapu66fLhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_func(query, response):\n",
        "    prompt = f\"\"\"Your job is to evaluate whether a machine learning model has hallucinated or not.\n",
        "    A hallucination occurs when the response is coherent but factually incorrect or nonsensical\n",
        "    outputs that are not grounded in the provided context.\n",
        "    You are given the following information:\n",
        "      ####INFO####\n",
        "      [Query]: {query}\n",
        "      [Model Response]: {response}\n",
        "      ####END INFO####\n",
        "      Based on the information provided is the model output a hallucination? Respond with only \"yes\" or \"no\"\n",
        "      \"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "NDVahFAtNpSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=merged_model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI-kx1_GMk3a",
        "outputId": "b4da28f8-5dbd-4871-a124-49b38c7aa9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_merged_model(query, response):\n",
        "  input = format_func(query, response)\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": input}\n",
        "  ]\n",
        "\n",
        "  pipe = pipeline(\n",
        "      \"text-generation\",\n",
        "      model=merged_model,\n",
        "      tokenizer=tokenizer,\n",
        "  )\n",
        "\n",
        "  generation_args = {\n",
        "      \"max_new_tokens\": 2,\n",
        "      \"return_full_text\": False,\n",
        "      \"temperature\": 0.01,\n",
        "      \"do_sample\": True,\n",
        "  }\n",
        "\n",
        "  output = pipe(messages, **generation_args)\n",
        "  torch.cuda.empty_cache()\n",
        "  return output[0]['generated_text'].strip().lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtBXjNl4Qmnw",
        "outputId": "fa6beaf5-16f6-49fc-d246-38297fc0db70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for Hallucination"
      ],
      "metadata": {
        "id": "h6qPehRJKLmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_merged_model(query='Based on the follwoing <context>Walrus are the largest mammal</context> answer the question <query> What is the best PC?</query>',response='The best PC is the mac')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ali4ffM5XQus",
        "outputId": "ff7d1b1b-4931-4572-c785-dc6fff6505d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}